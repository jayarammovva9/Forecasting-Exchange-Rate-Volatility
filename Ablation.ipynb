{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa4rgnsoLgEa"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# PART 0: INSTALL NECESSARY LIBRARIES\n",
        "# ===================================================================\n",
        "!pip install arch --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import io\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "\n",
        "# TensorFlow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Layer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Arch imports\n",
        "from arch import arch_model\n",
        "\n",
        "# ===================================================================\n",
        "# ABLATION STUDY PARAMETERS (CHANGE THESE FOR EACH RUN)\n",
        "# ===================================================================\n",
        "# --- Run 1: Baseline ---\n",
        "RUN_NAME = \"Baseline\"\n",
        "WINDOW_SIZE = 60\n",
        "LSTM_UNITS = [64, 32] # [Layer_1_Units, Layer_2_Units]\n",
        "\n",
        "# --- Run 2: Shorter Window ---\n",
        "# RUN_NAME = \"Shorter_Window_30\"\n",
        "# WINDOW_SIZE = 30\n",
        "# LSTM_UNITS = [64, 32]\n",
        "\n",
        "# --- Run 3: Simpler Architecture ---\n",
        "# RUN_NAME = \"Simpler_Arch\"\n",
        "# WINDOW_SIZE = 60\n",
        "# LSTM_UNITS = [32, 16]\n",
        "\n",
        "# --- Prototyping Settings (Set n_folds = 14 for final paper) ---\n",
        "n_folds = 4\n",
        "EPOCHS_PROTOTYPE = 50\n",
        "# ===================================================================\n",
        "\n",
        "# ===================================================================\n",
        "# PART 1: SETUP AND DATA LOADING\n",
        "# ===================================================================\n",
        "print(f\"--- STARTING RUN: {RUN_NAME} ---\")\n",
        "print(f\"Window Size: {WINDOW_SIZE}, LSTM Units: {LSTM_UNITS}\")\n",
        "\n",
        "# --- 1.1 Set Random Seed for Reproducibility ---\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "print(\"Global random seeds set.\")\n",
        "\n",
        "# --- 1.2 Load Data ---\n",
        "# Check if file exists in Colab, if not, upload\n",
        "file_name = 'final_predictors_2003_onwards.csv'\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Please upload '{file_name}' file.\")\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded))\n",
        "else:\n",
        "    print(f\"Using cached file: '{file_name}'\")\n",
        "    filename = file_name\n",
        "\n",
        "df = pd.read_csv(filename, index_col='Date', parse_dates=True)\n",
        "print(\"\\n--- Data Loaded Successfully ---\")\n",
        "\n",
        "# ===================================================================\n",
        "# PART 2: FEATURE ENGINEERING & TARGET VARIABLE\n",
        "# ===================================================================\n",
        "df['USDINR_log_return'] = np.log(df['USDINR'] / df['USDINR'].shift(1))\n",
        "df['SP500_log_return'] = np.log(df['SP500'] / df['SP500'].shift(1))\n",
        "df['OIL_WTI_log_return'] = np.log(df['OIL_WTI'] / df['OIL_WTI'].shift(1))\n",
        "df['GOLD_log_return'] = np.log(df['GOLD'] / df['GOLD'].shift(1))\n",
        "df['REALIZED_VOLATILITY'] = df['USDINR_log_return'].rolling(window=21).std() * np.sqrt(252)\n",
        "df['RV_d'] = df['REALIZED_VOLATILITY'].shift(1)\n",
        "df['RV_w'] = df['REALIZED_VOLATILITY'].shift(1).rolling(window=5).mean()\n",
        "df['RV_m'] = df['REALIZED_VOLATILITY'].shift(1).rolling(window=22).mean()\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# --- Define Feature Sets ---\n",
        "y = df['REALIZED_VOLATILITY']\n",
        "X_full = df.drop(columns=['REALIZED_VOLATILITY', 'USDINR', 'SP500', 'OIL_WTI', 'GOLD', 'USDINR_log_return'])\n",
        "X_simple = df[['RV_d', 'RV_w', 'RV_m']]\n",
        "\n",
        "# ===================================================================\n",
        "# PART 3: MODEL DEFINITIONS\n",
        "# ===================================================================\n",
        "# --- 3.1 LSTM Sequence Creation ---\n",
        "def create_sequences(X_data, y_data, window_size):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X_data) - window_size):\n",
        "        X_seq.append(X_data[i:(i + window_size)])\n",
        "        y_seq.append(y_data[i + window_size])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# --- 3.2 LSTM Model Builder ---\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs): super(Attention, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
        "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b); e = K.squeeze(e, axis=-1)\n",
        "        alpha = K.softmax(e); alpha = K.expand_dims(alpha, axis=-1)\n",
        "        context = x * alpha\n",
        "        return K.sum(context, axis=1)\n",
        "\n",
        "def build_lstm_model(window_size, n_features, lstm_units_list):\n",
        "    tf.random.set_seed(seed_value)\n",
        "    input_layer = Input(shape=(window_size, n_features))\n",
        "    # Use the parameters\n",
        "    lstm1 = LSTM(lstm_units_list[0], return_sequences=True)(input_layer)\n",
        "    dropout1 = Dropout(0.2)(lstm1)\n",
        "    lstm2 = LSTM(lstm_units_list[1], return_sequences=True)(dropout1)\n",
        "    attention = Attention()(lstm2)\n",
        "    dense1 = Dense(20, activation='relu')(attention)\n",
        "    output_layer = Dense(1)(dense1)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# ===================================================================\n",
        "# PART 4: WALK-FORWARD VALIDATION\n",
        "# ===================================================================\n",
        "print(\"\\n--- Starting Walk-Forward Validation ---\")\n",
        "\n",
        "# --- 4.1 Define Split Parameters ---\n",
        "split_index = int(len(df) * 0.8)\n",
        "train_df = df.iloc[:split_index]\n",
        "test_df = df.iloc[split_index:]\n",
        "\n",
        "n_test = len(test_df)\n",
        "FORECAST_CHUNK_SIZE = int(np.ceil(n_test / n_folds))\n",
        "print(f\"Test set size: {n_test} days. Forecasting in {n_folds} folds of ~{FORECAST_CHUNK_SIZE} days each.\")\n",
        "\n",
        "# --- 4.2 Create Lists to Store Results ---\n",
        "all_har_preds, all_garch_preds, all_gjr_preds, all_lstm_full_preds, all_lstm_simple_preds = [], [], [], [], []\n",
        "all_actuals = []\n",
        "\n",
        "# --- 4.3 Start the Walk-Forward Loop ---\n",
        "for i in range(n_folds):\n",
        "    # (The GARCH/HAR models are not part of the ablation, but we run them for comparison)\n",
        "    print(f\"\\n--- Fold {i+1}/{n_folds} ---\")\n",
        "    start_idx = i * FORECAST_CHUNK_SIZE\n",
        "    end_idx = min((i + 1) * FORECAST_CHUNK_SIZE, n_test)\n",
        "\n",
        "    current_test_chunk = test_df.iloc[start_idx:end_idx]\n",
        "    if current_test_chunk.empty: continue\n",
        "    current_train_df = df.iloc[:split_index + start_idx]\n",
        "\n",
        "    y_test_fold = y.loc[current_test_chunk.index]\n",
        "    all_actuals.append(y_test_fold)\n",
        "\n",
        "    # --- 4.3.1 HAR-RV Model ---\n",
        "    X_har_train = X_simple.loc[current_train_df.index.intersection(X_simple.index)]\n",
        "    y_har_train = y.loc[X_har_train.index]\n",
        "    X_har_test = X_simple.loc[current_test_chunk.index.intersection(X_simple.index)]\n",
        "\n",
        "    if not X_har_test.empty:\n",
        "        har_model = LinearRegression()\n",
        "        har_model.fit(X_har_train, y_har_train)\n",
        "        har_preds = har_model.predict(X_har_test)\n",
        "        all_har_preds.append(pd.Series(har_preds, index=X_har_test.index, name=\"HAR_Forecast\"))\n",
        "\n",
        "    # --- 4.3.3 **LSTM Models (Full and Simple)** ---\n",
        "    print(f\"Training LSTM models for Fold {i+1}...\")\n",
        "\n",
        "    # --- Full LSTM ---\n",
        "    X_train_loop_full = X_full.loc[current_train_df.index]\n",
        "    y_train_loop = y.loc[current_train_df.index]\n",
        "    X_test_loop_full = X_full.loc[current_test_chunk.index]\n",
        "    fs_full = MinMaxScaler(); ts_full = MinMaxScaler()\n",
        "    X_train_scaled_full = fs_full.fit_transform(X_train_loop_full)\n",
        "    X_test_scaled_full = fs_full.transform(X_test_loop_full)\n",
        "    y_train_scaled = ts_full.fit_transform(y_train_loop.values.reshape(-1, 1))\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train_scaled_full, y_train_scaled, WINDOW_SIZE)\n",
        "    combined_X_for_seq = np.concatenate((X_train_scaled_full[-WINDOW_SIZE:], X_test_scaled_full))\n",
        "    X_test_seq, _ = create_sequences(combined_X_for_seq, np.zeros(len(combined_X_for_seq)), WINDOW_SIZE)\n",
        "    X_test_seq_full = X_test_seq[:len(X_test_loop_full)]\n",
        "\n",
        "    model_full = build_lstm_model(WINDOW_SIZE, X_train_seq.shape[2], LSTM_UNITS)\n",
        "    model_full.fit(X_train_seq, y_train_seq, epochs=EPOCHS_PROTOTYPE, batch_size=32, validation_split=0.15, callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)], verbose=0)\n",
        "    lstm_preds_scaled = model_full.predict(X_test_seq_full)\n",
        "    lstm_preds_full = ts_full.inverse_transform(lstm_preds_scaled)\n",
        "    all_lstm_full_preds.append(pd.Series(lstm_preds_full.flatten(), index=y_test_fold.index, name=\"LSTM_Full_Forecast\"))\n",
        "\n",
        "    # --- Simple LSTM ---\n",
        "    X_train_loop_simple = X_simple.loc[current_train_df.index]\n",
        "    X_test_loop_simple = X_simple.loc[current_test_chunk.index]\n",
        "    fs_simple = MinMaxScaler(); ts_simple = MinMaxScaler()\n",
        "    X_train_scaled_simple = fs_simple.fit_transform(X_train_loop_simple)\n",
        "    X_test_scaled_simple = fs_simple.transform(X_test_loop_simple)\n",
        "    y_train_scaled_simple = ts_simple.fit_transform(y_train_loop.values.reshape(-1, 1))\n",
        "    X_train_seq_s, y_train_seq_s = create_sequences(X_train_scaled_simple, y_train_scaled_simple, WINDOW_SIZE)\n",
        "    combined_X_for_seq_s = np.concatenate((X_train_scaled_simple[-WINDOW_SIZE:], X_test_scaled_simple))\n",
        "    X_test_seq_s, _ = create_sequences(combined_X_for_seq_s, np.zeros(len(combined_X_for_seq_s)), WINDOW_SIZE)\n",
        "    X_test_seq_simple = X_test_seq_s[:len(X_test_loop_simple)]\n",
        "\n",
        "    model_simple = build_lstm_model(WINDOW_SIZE, X_train_seq_s.shape[2], LSTM_UNITS)\n",
        "    model_simple.fit(X_train_seq_s, y_train_seq_s, epochs=EPOCHS_PROTOTYPE, batch_size=32, validation_split=0.15, callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)], verbose=0)\n",
        "    lstm_preds_scaled_s = model_simple.predict(X_test_seq_simple)\n",
        "    lstm_preds_simple = ts_simple.inverse_transform(lstm_preds_scaled_s)\n",
        "    all_lstm_simple_preds.append(pd.Series(lstm_preds_simple.flatten(), index=y_test_fold.index, name=\"LSTM_Simple_Forecast\"))\n",
        "\n",
        "print(\"\\n--- Walk-Forward Validation Complete ---\")\n",
        "\n",
        "# ===================================================================\n",
        "# PART 5: RESULTS AND EVALUATION\n",
        "# ===================================================================\n",
        "print(\"\\n--- Comparing Model Performance ---\")\n",
        "\n",
        "# --- 5.1 Combine all results ---\n",
        "har_results = pd.concat(all_har_preds)\n",
        "lstm_full_results = pd.concat(all_lstm_full_preds)\n",
        "lstm_simple_results = pd.concat(all_lstm_simple_preds)\n",
        "actuals_results = pd.concat(all_actuals); actuals_results.name = 'REALIZED_VOLATILITY'\n",
        "\n",
        "results_df = pd.DataFrame(actuals_results)\n",
        "results_df = results_df.join(har_results, how='left')\n",
        "results_df = results_df.join(lstm_full_results, how='left')\n",
        "results_df = results_df.join(lstm_simple_results, how='left')\n",
        "results_df.dropna(inplace=True)\n",
        "# Save results for this run\n",
        "results_df.to_csv(f'results_{RUN_NAME}.csv')\n",
        "print(f\"Results for this run saved to 'results_{RUN_NAME}.csv'\")\n",
        "\n",
        "# --- 5.2 Calculate and Print Metrics ---\n",
        "models_to_evaluate = [\"HAR_Forecast\", \"LSTM_Full_Forecast\", \"LSTM_Simple_Forecast\"]\n",
        "metrics = {'Model': [], 'RMSE': [], 'MAE': []}\n",
        "for model_name in models_to_evaluate:\n",
        "    rmse = np.sqrt(mean_squared_error(results_df['REALIZED_VOLATILITY'], results_df[model_name]))\n",
        "    mae = mean_absolute_error(results_df['REALIZED_VOLATILITY'], results_df[model_name])\n",
        "    metrics['Model'].append(model_name)\n",
        "    metrics['RMSE'].append(rmse)\n",
        "    metrics['MAE'].append(mae)\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "print(f\"\\n--- Final Model Metrics ({RUN_NAME}) ---\")\n",
        "print(metrics_df.round(6))"
      ]
    }
  ]
}